{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import importlib\n",
    "import IPython.display as ipd\n",
    "\n",
    "import util_audio_preprocess\n",
    "import util_audio_transform\n",
    "import util_auditory_model_loss\n",
    "import util_cochlear_model\n",
    "import util_recognition_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob('data/toy_dataset*.tfrecords')\n",
    "batch_size = 8\n",
    "feature_description = {\n",
    "    'background/signal': tf.io.FixedLenFeature([], tf.string, default_value=None),\n",
    "    'foreground/signal': tf.io.FixedLenFeature([], tf.string, default_value=None),\n",
    "}\n",
    "bytes_description = {\n",
    "    'background/signal': {'dtype': tf.float32, 'shape': [40000]}, \n",
    "    'foreground/signal': {'dtype': tf.float32, 'shape': [40000]},\n",
    "}\n",
    "\n",
    "\n",
    "def parse_tfrecord(tfrecord):\n",
    "    tfrecord = tf.parse_single_example(tfrecord, features=feature_description)\n",
    "    for key in bytes_description.keys():\n",
    "        tfrecord[key] = tf.decode_raw(tfrecord[key], bytes_description[key]['dtype'])\n",
    "        tfrecord[key] = tf.reshape(tfrecord[key], bytes_description[key]['shape'])\n",
    "    return tfrecord\n",
    "\n",
    "\n",
    "def preprocess_audio_batch(batch):\n",
    "    \"\"\"\n",
    "    Function combines foreground (speech) and background (noise) audio\n",
    "    signals with signal-to-noise ratios drawn uniformly between -20 and\n",
    "    +10 dB. The returned dictionary contains the noisy speech signal,\n",
    "    the clean speech signal, and the SNR.\n",
    "    \"\"\"\n",
    "    foreground_signal = batch['foreground/signal']\n",
    "    background_signal = batch['background/signal']\n",
    "    snr = tf.random.uniform(\n",
    "        [tf.shape(foreground_signal)[0], 1],\n",
    "        minval=-20.0,\n",
    "        maxval=10.0,\n",
    "        dtype=foreground_signal.dtype)\n",
    "    signal_in_noise, signal, noise_scaled = util_audio_preprocess.tf_set_snr(\n",
    "        foreground_signal,\n",
    "        background_signal,\n",
    "        snr)\n",
    "    batch = {\n",
    "        'snr': snr,\n",
    "        'waveform_noisy': signal_in_noise,\n",
    "        'waveform_clean': signal,\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.random.set_random_seed(0)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(filenames=filenames, compression_type='GZIP')\n",
    "dataset = dataset.map(parse_tfrecord)\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.map(preprocess_audio_batch)\n",
    "dataset = dataset.prefetch(buffer_size=4)\n",
    "dataset = dataset.shuffle(buffer_size=32)\n",
    "dataset = dataset.repeat(count=None)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "input_tensor_dict = iterator.get_next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 recognition networks included for deep feature loss:\n",
      "|__ arch1_taskA: models/recognition_networks/arch1_taskA.ckpt-550000\n",
      "Building waveform loss\n",
      "Building cochlear model loss\n",
      "[make_cos_filters_nx] using filter_spacing=`erb`\n",
      "[make_cos_filters_nx] using filter_spacing=`erb`\n",
      "Building deep feature loss (recognition network: arch1_taskA)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'separator/conv1d/kernel:0' shape=(15, 1, 24) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d/bias:0' shape=(24,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_1/kernel:0' shape=(15, 24, 48) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_1/bias:0' shape=(48,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_2/kernel:0' shape=(15, 48, 72) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_2/bias:0' shape=(72,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_3/kernel:0' shape=(15, 72, 96) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_3/bias:0' shape=(96,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_4/kernel:0' shape=(15, 96, 120) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_4/bias:0' shape=(120,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_5/kernel:0' shape=(15, 120, 144) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_5/bias:0' shape=(144,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_6/kernel:0' shape=(15, 144, 168) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_6/bias:0' shape=(168,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_7/kernel:0' shape=(15, 168, 192) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_7/bias:0' shape=(192,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_8/kernel:0' shape=(15, 192, 216) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_8/bias:0' shape=(216,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_9/kernel:0' shape=(15, 216, 240) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_9/bias:0' shape=(240,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_10/kernel:0' shape=(15, 240, 264) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_10/bias:0' shape=(264,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_11/kernel:0' shape=(15, 264, 288) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_11/bias:0' shape=(288,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_12/kernel:0' shape=(15, 288, 312) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_12/bias:0' shape=(312,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/interp_0:0' shape=(312,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_13/kernel:0' shape=(5, 600, 288) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_13/bias:0' shape=(288,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/interp_1:0' shape=(288,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_14/kernel:0' shape=(5, 552, 264) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_14/bias:0' shape=(264,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/interp_2:0' shape=(264,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_15/kernel:0' shape=(5, 504, 240) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_15/bias:0' shape=(240,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/interp_3:0' shape=(240,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_16/kernel:0' shape=(5, 456, 216) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_16/bias:0' shape=(216,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/interp_4:0' shape=(216,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_17/kernel:0' shape=(5, 408, 192) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_17/bias:0' shape=(192,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/interp_5:0' shape=(192,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_18/kernel:0' shape=(5, 360, 168) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_18/bias:0' shape=(168,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/interp_6:0' shape=(168,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_19/kernel:0' shape=(5, 312, 144) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_19/bias:0' shape=(144,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/interp_7:0' shape=(144,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_20/kernel:0' shape=(5, 264, 120) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_20/bias:0' shape=(120,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/interp_8:0' shape=(120,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_21/kernel:0' shape=(5, 216, 96) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_21/bias:0' shape=(96,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/interp_9:0' shape=(96,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_22/kernel:0' shape=(5, 168, 72) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_22/bias:0' shape=(72,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/interp_10:0' shape=(72,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_23/kernel:0' shape=(5, 120, 48) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_23/bias:0' shape=(48,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/interp_11:0' shape=(48,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_24/kernel:0' shape=(5, 72, 24) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_24/bias:0' shape=(24,) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_25/kernel:0' shape=(1, 25, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'separator/conv1d_25/bias:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_waveform_noisy = input_tensor_dict['waveform_noisy']\n",
    "tensor_waveform_clean = input_tensor_dict['waveform_clean']\n",
    "tensor_waveform_denoised = util_audio_transform.build_unet(tensor_waveform_noisy)\n",
    "\n",
    "list_recognition_networks = [\n",
    "    'arch1_taskA',\n",
    "#     'arch2_taskA',\n",
    "#     'arch3_taskA',\n",
    "]\n",
    "auditory_model = util_auditory_model_loss.AuditoryModelLoss(\n",
    "    list_recognition_networks=list_recognition_networks,\n",
    "    tensor_wave0=tensor_waveform_clean,\n",
    "    tensor_wave1=tensor_waveform_denoised)\n",
    "\n",
    "transform_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='separator')\n",
    "transform_saver = tf.train.Saver(var_list=transform_var_list, max_to_keep=0)\n",
    "\n",
    "# loss = auditory_model.loss_waveform\n",
    "loss = auditory_model.loss_cochlear_model\n",
    "# loss = auditory_model.loss_deep_features\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "train_op = optimizer.minimize(\n",
    "    loss=loss,\n",
    "    global_step=None,\n",
    "    var_list=transform_var_list)\n",
    "\n",
    "transform_var_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading `arch1_taskA` variables from models/recognition_networks/arch1_taskA.ckpt-550000\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from models/recognition_networks/arch1_taskA.ckpt-550000\n",
      "INFO:tensorflow:Restoring parameters from models/recognition_networks/arch1_taskA.ckpt-550000\n",
      "Loss after training step 000000 = 552.91\n",
      "Loss after training step 000005 = 652.04\n",
      "Loss after training step 000010 = 456.93\n",
      "Loss after training step 000015 = 491.50\n",
      "Loss after training step 000020 = 488.89\n",
      "Loss after training step 000025 = 457.64\n",
      "Loss after training step 000030 = 455.87\n",
      "Loss after training step 000035 = 448.33\n",
      "Loss after training step 000040 = 447.42\n",
      "Loss after training step 000045 = 460.21\n",
      "Loss after training step 000050 = 436.42\n",
      "Loss after training step 000055 = 493.47\n",
      "Loss after training step 000060 = 457.38\n",
      "Loss after training step 000065 = 441.01\n",
      "Loss after training step 000070 = 420.04\n",
      "Loss after training step 000075 = 428.37\n",
      "Loss after training step 000080 = 437.19\n",
      "Loss after training step 000085 = 437.20\n",
      "Loss after training step 000090 = 444.52\n",
      "Loss after training step 000095 = 428.84\n",
      "Loss after training step 000100 = 397.66\n",
      "INFO:tensorflow:new_model.ckpt-100 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    auditory_model.load_auditory_model_vars(sess)\n",
    "    \n",
    "    for step in range(101):\n",
    "        _, step_loss = sess.run([train_op, loss])\n",
    "        if step % 5 == 0:\n",
    "            print(\"Loss after training step {:06d} = {:.02f}\".format(step, step_loss.mean()))\n",
    "    \n",
    "    transform_saver.save(\n",
    "        sess,\n",
    "        save_path='new_model.ckpt',\n",
    "        global_step=step,\n",
    "        write_meta_graph=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
