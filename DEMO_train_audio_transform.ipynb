{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import importlib\n",
    "import IPython.display as ipd\n",
    "\n",
    "import util_audio_preprocess\n",
    "import util_audio_transform\n",
    "import util_auditory_model_loss\n",
    "import util_cochlear_model\n",
    "import util_recognition_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data inputput pipeline\n",
    "\n",
    "Recommended training data format is tfrecords files containing foreground\n",
    "(speech) and background (noise) waveforms stored separately. Model was\n",
    "originally trained with 2-second audio clips (20 kHz sampling rate).\n",
    "\n",
    "NOTE: the full dataset used to train models in the paper is not included in\n",
    "this code release as it was compiled from previously published datasets\n",
    "(speech from Wall Street Journal and Spoken Wikipedia Corpora; background\n",
    "noise from Audioset). Data available upon request to authors.\n",
    "\"\"\"\n",
    "\n",
    "filenames = glob.glob('data/toy_dataset*.tfrecords')\n",
    "batch_size = 8\n",
    "feature_description = {\n",
    "    'background/signal': tf.io.FixedLenFeature([], tf.string, default_value=None),\n",
    "    'foreground/signal': tf.io.FixedLenFeature([], tf.string, default_value=None),\n",
    "}\n",
    "bytes_description = {\n",
    "    'background/signal': {'dtype': tf.float32, 'shape': [40000]}, \n",
    "    'foreground/signal': {'dtype': tf.float32, 'shape': [40000]},\n",
    "}\n",
    "\n",
    "\n",
    "def parse_tfrecord(tfrecord):\n",
    "    tfrecord = tf.parse_single_example(tfrecord, features=feature_description)\n",
    "    for key in bytes_description.keys():\n",
    "        tfrecord[key] = tf.decode_raw(tfrecord[key], bytes_description[key]['dtype'])\n",
    "        tfrecord[key] = tf.reshape(tfrecord[key], bytes_description[key]['shape'])\n",
    "    return tfrecord\n",
    "\n",
    "\n",
    "def preprocess_audio_batch(batch):\n",
    "    \"\"\"\n",
    "    Function combines foreground (speech) and background (noise) audio\n",
    "    signals with signal-to-noise ratios drawn uniformly between -20 and\n",
    "    +10 dB. The returned dictionary contains the noisy speech signal,\n",
    "    the clean speech signal, and the SNR.\n",
    "    \"\"\"\n",
    "    foreground_signal = batch['foreground/signal']\n",
    "    background_signal = batch['background/signal']\n",
    "    snr = tf.random.uniform(\n",
    "        [tf.shape(foreground_signal)[0], 1],\n",
    "        minval=-20.0,\n",
    "        maxval=10.0,\n",
    "        dtype=foreground_signal.dtype)\n",
    "    signal_in_noise, signal, noise_scaled = util_audio_preprocess.tf_set_snr(\n",
    "        foreground_signal,\n",
    "        background_signal,\n",
    "        snr)\n",
    "    batch = {\n",
    "        'snr': snr,\n",
    "        'waveform_noisy': signal_in_noise,\n",
    "        'waveform_clean': signal,\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.random.set_random_seed(0)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(filenames=filenames, compression_type='GZIP')\n",
    "dataset = dataset.map(parse_tfrecord)\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.map(preprocess_audio_batch)\n",
    "dataset = dataset.prefetch(buffer_size=4)\n",
    "dataset = dataset.shuffle(buffer_size=32)\n",
    "dataset = dataset.repeat(count=None)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "input_tensor_dict = iterator.get_next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From Wave-U-Net/UnetAudioSeparator.py:97: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv1d instead.\n",
      "1 recognition networks included for deep feature loss:\n",
      "|__ arch1_taskA: models/recognition_networks/arch1_taskA.ckpt-550000\n",
      "Building waveform loss\n",
      "Building cochlear model loss\n",
      "[make_cos_filters_nx] using filter_spacing=`erb`\n",
      "[make_cos_filters_nx] using filter_spacing=`erb`\n",
      "Building deep feature loss (recognition network: arch1_taskA)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/signal/fft_ops.py:315: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model training graph\n",
    "\n",
    "Components:\n",
    "1. U-Net audio transform (`util_audio_transform.build_unet`)\n",
    "2. Auditory model loss function (`util_auditory_model_loss.AuditoryModelLoss`)\n",
    "3. Tensorflow optimizer to train U-Net weights\n",
    "\"\"\"\n",
    "\n",
    "### U-Net audio transform\n",
    "tensor_waveform_noisy = input_tensor_dict['waveform_noisy']\n",
    "tensor_waveform_clean = input_tensor_dict['waveform_clean']\n",
    "tensor_waveform_denoised = util_audio_transform.build_unet(tensor_waveform_noisy)\n",
    "\n",
    "### Build auditory model loss function (specify recognition networks\n",
    "### to include in the deep feature loss)\n",
    "list_recognition_networks = [\n",
    "    'arch1_taskA',\n",
    "#     'arch2_taskA',\n",
    "#     'arch3_taskA',\n",
    "]\n",
    "auditory_model = util_auditory_model_loss.AuditoryModelLoss(\n",
    "    list_recognition_networks=list_recognition_networks,\n",
    "    tensor_wave0=tensor_waveform_clean,\n",
    "    tensor_wave1=tensor_waveform_denoised)\n",
    "\n",
    "transform_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='separator')\n",
    "transform_saver = tf.train.Saver(var_list=transform_var_list, max_to_keep=0)\n",
    "\n",
    "### Specify loss function (waveform, cochlear model, or deep features)\n",
    "### and build optimizer object + training operation\n",
    "loss = auditory_model.loss_cochlear_model # <-- cochlear model loss is lightweight and works well\n",
    "# loss = auditory_model.loss_deep_features\n",
    "# loss = auditory_model.loss_waveform\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "train_op = optimizer.minimize(\n",
    "    loss=loss,\n",
    "    global_step=None,\n",
    "    var_list=transform_var_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading `arch1_taskA` variables from models/recognition_networks/arch1_taskA.ckpt-550000\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from models/recognition_networks/arch1_taskA.ckpt-550000\n",
      "INFO:tensorflow:Restoring parameters from models/recognition_networks/arch1_taskA.ckpt-550000\n",
      "Loss after training step 000000 = 759.71\n",
      "Loss after training step 000010 = 594.60\n",
      "Loss after training step 000020 = 547.26\n",
      "Loss after training step 000030 = 454.63\n",
      "Loss after training step 000040 = 478.92\n",
      "Loss after training step 000050 = 450.72\n",
      "Loss after training step 000060 = 395.44\n",
      "Loss after training step 000070 = 427.17\n",
      "Loss after training step 000080 = 456.67\n",
      "Loss after training step 000090 = 438.67\n",
      "Loss after training step 000100 = 431.82\n",
      "INFO:tensorflow:new_model.ckpt-100 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple training routine\n",
    "\n",
    "Models in the paper were all trained for 600000 steps\n",
    "with batch size 8 and learning rate 10e-4.\n",
    "\"\"\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    auditory_model.load_auditory_model_vars(sess)\n",
    "    \n",
    "    for step in range(101):\n",
    "        _, step_loss = sess.run([train_op, loss])\n",
    "        if step % 10 == 0:\n",
    "            print(\"Loss after training step {:06d} = {:.02f}\".format(step, step_loss.mean()))\n",
    "    \n",
    "    transform_saver.save(\n",
    "        sess,\n",
    "        save_path='new_model.ckpt',\n",
    "        global_step=step,\n",
    "        write_meta_graph=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
