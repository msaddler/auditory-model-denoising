{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io.wavfile\n",
    "import IPython.display as ipd\n",
    "\n",
    "import util_recognition_network\n",
    "import util_cochlear_model\n",
    "import util_auditory_model_loss\n",
    "import util_audio_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio example: audio/ex01_unprocessed_input.wav\n",
      "Loading audio example: audio/ex02_unprocessed_input.wav\n",
      "Loading audio example: audio/ex03_unprocessed_input.wav\n",
      "Loading audio example: audio/ex04_unprocessed_input.wav\n",
      "Loading audio example: audio/ex05_unprocessed_input.wav\n",
      "Loading audio example: audio/ex06_unprocessed_input.wav\n",
      "Loading audio example: audio/ex07_unprocessed_input.wav\n",
      "Loading audio example: audio/ex08_unprocessed_input.wav\n",
      "Loading audio example: audio/ex09_unprocessed_input.wav\n",
      "Loading audio example: audio/ex10_unprocessed_input.wav\n"
     ]
    }
   ],
   "source": [
    "def get_rms(y):\n",
    "    return np.sqrt(np.mean(np.square(y - np.mean(y))))\n",
    "\n",
    "def set_rms(y, rms):\n",
    "    y = y - np.mean(y)\n",
    "    return rms * y / get_rms(y)\n",
    "\n",
    "regex_fn_wav = 'audio/ex*_unprocessed_input.wav'\n",
    "list_fn_wav = glob.glob(regex_fn_wav)\n",
    "\n",
    "list_wav = []\n",
    "for fn_wav in list_fn_wav:\n",
    "    print('Loading audio example: {}'.format(fn_wav))\n",
    "    sr, y = scipy.io.wavfile.read(fn_wav)\n",
    "    y = set_rms(y, 0.02)\n",
    "    assert sr == 20e3\n",
    "    list_wav.append(y)\n",
    "list_wav = np.array(list_wav)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_audio_transform_tag = [\n",
    "    'unet_A1',\n",
    "    'unet_A123',\n",
    "    'unet_A123W123',\n",
    "    'unet_A1W1',\n",
    "    'unet_R1',\n",
    "    'unet_R123',\n",
    "    'unet_W1',\n",
    "    'unet_W123',\n",
    "    'unet_cochlear_human',\n",
    "    'unet_cochlear_reverse',\n",
    "    'unet_germain_deep_features',\n",
    "    'unet_waveform',\n",
    "]\n",
    "augio_transform_tag = list_audio_transform_tag[0]\n",
    "checkpoint_filename = 'models/audio_transforms/{}/model.ckpt-600000'.format(augio_transform_tag)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "placeholder_wav = tf.placeholder(tf.float32, [None, 40000])\n",
    "tensor_wav_unet = util_audio_transform.build_unet(\n",
    "    placeholder_wav,\n",
    "    signal_rate=20000,\n",
    "    UNET_PARAMS={})\n",
    "var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=None)\n",
    "saver = tf.train.Saver(var_list=var_list, max_to_keep=0)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Loading audio transform variables from: {}'.format(checkpoint_filename))\n",
    "    saver.restore(sess, checkpoint_filename)\n",
    "    print('Running audio transform: {}'.format(augio_transform_tag))\n",
    "    list_wav_unet = sess.run(tensor_wav_unet, feed_dict={placeholder_wav: list_wav})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for itr_wav in range(len(list_fn_wav)):\n",
    "    print('======================== Example {:02d} ========================'.format(itr_wav + 1))\n",
    "    print('Unprocessed input audio ({}):'.format(list_fn_wav[itr_wav]))\n",
    "    ipd.display(ipd.Audio(list_wav[itr_wav], rate=20e3))\n",
    "    print('Processed audio ({}):'.format(augio_transform_tag))\n",
    "    ipd.display(ipd.Audio(list_wav_unet[itr_wav], rate=20e3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 recognition networks included for deep feature loss:\n",
      "|__ arch1_taskR: models/recognition_networks/arch1_taskR.ckpt-0\n",
      "|__ arch2_taskR: models/recognition_networks/arch2_taskR.ckpt-0\n",
      "|__ arch3_taskR: models/recognition_networks/arch3_taskR.ckpt-0\n",
      "Building waveform loss\n",
      "Building cochlear model loss\n",
      "[make_cos_filters_nx] using filter_spacing=`erb`\n",
      "[make_cos_filters_nx] using filter_spacing=`erb`\n",
      "Building deep feature loss (recognition network: arch1_taskR)\n",
      "Building deep feature loss (recognition network: arch2_taskR)\n",
      "Building deep feature loss (recognition network: arch3_taskR)\n",
      "Loading `arch1_taskR` variables from models/recognition_networks/arch1_taskR.ckpt-0\n",
      "Loading `arch2_taskR` variables from models/recognition_networks/arch2_taskR.ckpt-0\n",
      "Loading `arch3_taskR` variables from models/recognition_networks/arch3_taskR.ckpt-0\n",
      "[872.1858]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1078.2817]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1.1398318]\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import importlib\n",
    "importlib.reload(util_auditory_model_loss)\n",
    "\n",
    "\n",
    "list_recognition_networks = [\n",
    "#     'arch1_taskA',\n",
    "    'arch1_taskR',\n",
    "#     'arch1_taskW',\n",
    "#     'arch2_taskA',\n",
    "    'arch2_taskR',\n",
    "#     'arch2_taskW',\n",
    "#     'arch3_taskA',\n",
    "    'arch3_taskR',\n",
    "#     'arch3_taskW',\n",
    "]\n",
    "\n",
    "auditory_model_loss = util_auditory_model_loss.AuditoryModelLoss(\n",
    "    list_recognition_networks=list_recognition_networks)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    auditory_model_loss.load_auditory_model_vars(sess)\n",
    "\n",
    "    print(auditory_model_loss.waveform_loss(list_wav[0:1], list_wav[1:2]))\n",
    "    print(auditory_model_loss.waveform_loss(list_wav, list_wav))\n",
    "    print(auditory_model_loss.cochlear_model_loss(list_wav[0:1], list_wav[1:2]))\n",
    "    print(auditory_model_loss.cochlear_model_loss(list_wav, list_wav))\n",
    "    print(auditory_model_loss.deep_feature_loss(list_wav[0:1], list_wav[1:2]))\n",
    "    print(auditory_model_loss.deep_feature_loss(list_wav[0:1], list_wav[0:1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
