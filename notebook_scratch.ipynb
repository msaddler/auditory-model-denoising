{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io.wavfile\n",
    "import IPython.display as ipd\n",
    "\n",
    "import util_recognition_network\n",
    "import util_cochlear_model\n",
    "import util_audio_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCH = 1\n",
    "TASK = 'A'\n",
    "\n",
    "fn_ckpt = 'models/recognition_networks/arch{}_task{}.ckpt*'.format(ARCH, TASK)\n",
    "fn_ckpt = glob.glob(fn_ckpt)[-1].replace('.index', '')\n",
    "fn_arch = 'models/recognition_networks/arch{}.json'.format(ARCH)\n",
    "with open(fn_arch, 'r') as f_arch:\n",
    "    list_layer_dict = json.load(f_arch)\n",
    "\n",
    "fn_ckpt, fn_arch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "importlib.reload(util_recognition_network)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "if 'taskA' in fn_ckpt:\n",
    "    n_classes_dict = {\"/stimuli/labels_binary_via_int\": 517}\n",
    "else:\n",
    "    n_classes_dict = {\"/stimuli/word_int\": 794}\n",
    "\n",
    "\n",
    "tensor_input = tf.placeholder(tf.float32, [None, 40, 20000, 1])\n",
    "tensor_output, tensors = util_recognition_network.build_network(\n",
    "    tensor_input,\n",
    "    list_layer_dict,\n",
    "    n_classes_dict=n_classes_dict)\n",
    "\n",
    "var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=None)\n",
    "var_dict = {v.name: v for v in var_list}\n",
    "\n",
    "saver = tf.train.Saver(var_list=var_list, max_to_keep=0)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    var_dict_init = sess.run(var_dict)\n",
    "    saver.restore(sess, fn_ckpt)\n",
    "    var_dict_load = sess.run(var_dict)\n",
    "\n",
    "for v in var_list:\n",
    "    print(v.name, v.shape, np.mean(np.abs(var_dict_init[v.name] - var_dict_load[v.name])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rms(y):\n",
    "    return np.sqrt(np.mean(np.square(y - np.mean(y))))\n",
    "\n",
    "def set_rms(y, rms):\n",
    "    y = y - np.mean(y)\n",
    "    return rms * y / get_rms(y)\n",
    "\n",
    "\n",
    "regex_fn_wav = 'audio/ex*_unprocessed_input.wav'\n",
    "list_fn_wav = glob.glob(regex_fn_wav)\n",
    "\n",
    "list_y = []\n",
    "for fn_wav in list_fn_wav:\n",
    "    sr, y = scipy.io.wavfile.read(fn_wav)\n",
    "    y = set_rms(y, 0.02)\n",
    "    assert sr == 20e3\n",
    "    list_y.append(y)\n",
    "list_y = np.array(list_y)\n",
    "list_y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(util_cochlear_model)\n",
    "importlib.reload(util_audio_transform)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "fn_ckpt = 'models/audio_transforms/unet_cochlear_reverse/model.ckpt-600000'\n",
    "\n",
    "tensor_waveform = tf.placeholder(tf.float32, [None, 40000])\n",
    "\n",
    "tensor_cochlear_representation, coch_container = util_cochlear_model.build_cochlear_model(\n",
    "    tensor_waveform,\n",
    "    signal_rate=20000,\n",
    "    filter_type='half-cosine',\n",
    "    filter_spacing='erb',\n",
    "    HIGH_LIM=8000,\n",
    "    LOW_LIM=20,\n",
    "    N=40,\n",
    "    SAMPLE_FACTOR=1,\n",
    "    bandwidth_scale_factor=1.0,\n",
    "    compression='stable_point3',\n",
    "    include_highpass=False,\n",
    "    include_lowpass=False,\n",
    "    linear_max=1.0,\n",
    "    rFFT=True,\n",
    "    rectify_and_lowpass_subbands=True,\n",
    "    return_subbands_only=True)\n",
    "\n",
    "tensor_waveform_unet = util_audio_transform.build_unet(\n",
    "    tensor_waveform,\n",
    "    signal_rate=20000,\n",
    "    UNET_PARAMS={})\n",
    "\n",
    "var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=None)\n",
    "saver = tf.train.Saver(var_list=var_list, max_to_keep=0)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, fn_ckpt)\n",
    "    list_y_unet = sess.run(tensor_waveform_unet, feed_dict={tensor_waveform: list_y})\n",
    "    list_coch_y = sess.run(tensor_cochlear_representation, feed_dict={tensor_waveform: list_y})\n",
    "    list_coch_y_unet = sess.run(tensor_cochlear_representation, feed_dict={tensor_waveform: list_y_unet})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for itr0 in range(list_y_unet.shape[0]):\n",
    "    y = list_y[itr0]\n",
    "    y_unet = list_y_unet[itr0]\n",
    "    print(list_fn_wav[itr0], np.mean(list_coch_y[itr0]), np.mean(list_coch_y_unet[itr0]))\n",
    "    ipd.display(ipd.Audio(y, rate=sr))\n",
    "    ipd.display(ipd.Audio(y_unet, rate=sr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arch1_taskA': {'fn_arch': 'models/recognition_networks/arch1.json',\n",
       "  'fn_ckpt': 'models/recognition_networks/arch1_taskA.ckpt-550000',\n",
       "  'n_classes_dict': {'task_audioset': 517}},\n",
       " 'arch2_taskA': {'fn_arch': 'models/recognition_networks/arch2.json',\n",
       "  'fn_ckpt': 'models/recognition_networks/arch2_taskA.ckpt-950000',\n",
       "  'n_classes_dict': {'task_audioset': 517}},\n",
       " 'arch3_taskA': {'fn_arch': 'models/recognition_networks/arch3.json',\n",
       "  'fn_ckpt': 'models/recognition_networks/arch3_taskA.ckpt-980000',\n",
       "  'n_classes_dict': {'task_audioset': 517}}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_recognition_network_ckpt = 'models/recognition_networks/arch*taskA*ckpt*index'\n",
    "fn_deep_feature_loss_weights= 'models/recognition_networks/deep_feature_loss_weights.json'\n",
    "\n",
    "list_recognition_network_ckpt = [\n",
    "    fn_ckpt.replace('.index', '') for fn_ckpt in glob.glob(regex_recognition_network_ckpt)\n",
    "]\n",
    "\n",
    "with open(fn_deep_feature_loss_weights, 'r') as f:\n",
    "    deep_feature_loss_weights = json.load(f)\n",
    "\n",
    "dict_recognition_network = {}\n",
    "for fn_ckpt in list_recognition_network_ckpt:\n",
    "    key = os.path.basename(fn_ckpt).split('.')[0]\n",
    "    if 'taskA' in key:\n",
    "        n_classes_dict = {\"task_audioset\": 517}\n",
    "    else:\n",
    "        n_classes_dict = {\"task_word\": 794}\n",
    "    dict_recognition_network[key] = {\n",
    "        'fn_ckpt': fn_ckpt,\n",
    "        'fn_arch': fn_ckpt[:fn_ckpt.rfind('_task')] + '.json',\n",
    "        'n_classes_dict': n_classes_dict,\n",
    "    }\n",
    "\n",
    "dict_recognition_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[make_cos_filters_nx] using filter_spacing=`erb`\n",
      "[make_cos_filters_nx] using filter_spacing=`erb`\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(util_audio_transform)\n",
    "importlib.reload(util_cochlear_model)\n",
    "importlib.reload(util_recognition_network)\n",
    "\n",
    "\n",
    "kwargs_build_cochlear_model = {}\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tensor_waveform_0 = tf.placeholder(tf.float32, [None, 40000])\n",
    "tensor_waveform_1 = tf.placeholder(tf.float32, [None, 40000])\n",
    "tensor_deep_feature_loss = tf.zeros([], dtype=tf.float32)\n",
    "\n",
    "tensor_coch_0, _ = util_cochlear_model.build_cochlear_model(\n",
    "    tensor_waveform_0,\n",
    "    **kwargs_build_cochlear_model)\n",
    "tensor_coch_1, _ = util_cochlear_model.build_cochlear_model(\n",
    "    tensor_waveform_1,\n",
    "    **kwargs_build_cochlear_model)\n",
    "\n",
    "\n",
    "for recognition_network_key in sorted(dict_recognition_network.keys()):\n",
    "    with open(dict_recognition_network[recognition_network_key]['fn_arch'], 'r') as f:\n",
    "        list_layer_dict = json.load(f)\n",
    "    with tf.variable_scope(recognition_network_key + '_0') as scope:\n",
    "        _, recognition_network_tensors_0 = util_recognition_network.build_network(\n",
    "            tensor_coch_0,\n",
    "            list_layer_dict,\n",
    "            n_classes_dict=dict_recognition_network[key]['n_classes_dict'])\n",
    "        var_list = {\n",
    "            v.name.replace(scope.name + '/', '').replace(':0', ''): v\n",
    "            for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope.name)\n",
    "        }\n",
    "        dict_recognition_network[recognition_network_key]['saver_0'] = tf.train.Saver(\n",
    "            var_list=var_list,\n",
    "            max_to_keep=0)\n",
    "    with tf.variable_scope(recognition_network_key + '_1') as scope:\n",
    "        _, recognition_network_tensors_1 = util_recognition_network.build_network(\n",
    "            tensor_coch_1,\n",
    "            list_layer_dict,\n",
    "            n_classes_dict=dict_recognition_network[key]['n_classes_dict'])\n",
    "        var_list = {\n",
    "            v.name.replace(scope.name + '/', '').replace(':0', ''): v\n",
    "            for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope.name)\n",
    "        }\n",
    "        dict_recognition_network[recognition_network_key]['saver_1'] = tf.train.Saver(\n",
    "            var_list=var_list,\n",
    "            max_to_keep=0)\n",
    "    \n",
    "    for feature_key in sorted(deep_feature_loss_weights[recognition_network_key].keys()):\n",
    "        feature_weight = deep_feature_loss_weights[recognition_network_key][feature_key]\n",
    "        feature_0 = recognition_network_tensors_0[feature_key]\n",
    "        feature_1 = recognition_network_tensors_1[feature_key]\n",
    "\n",
    "        feature_l1_distance = tf.reduce_sum(\n",
    "            tf.math.abs(feature_0 - feature_1),\n",
    "            axis=np.arange(1, len(feature_0.get_shape().as_list())))\n",
    "\n",
    "        tensor_deep_feature_loss += feature_weight * feature_l1_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/recognition_networks/arch1_taskA.ckpt-550000\n",
      "INFO:tensorflow:Restoring parameters from models/recognition_networks/arch1_taskA.ckpt-550000\n",
      "INFO:tensorflow:Restoring parameters from models/recognition_networks/arch2_taskA.ckpt-950000\n",
      "INFO:tensorflow:Restoring parameters from models/recognition_networks/arch2_taskA.ckpt-950000\n",
      "INFO:tensorflow:Restoring parameters from models/recognition_networks/arch3_taskA.ckpt-980000\n",
      "INFO:tensorflow:Restoring parameters from models/recognition_networks/arch3_taskA.ckpt-980000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([49.191277, 48.231163, 48.53233 , 48.74688 , 48.85193 , 48.27629 ,\n",
       "       48.18045 , 48.591633, 47.91723 , 47.967995], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rms(y):\n",
    "    return np.sqrt(np.mean(np.square(y - np.mean(y))))\n",
    "\n",
    "def set_rms(y, rms):\n",
    "    y = y - np.mean(y)\n",
    "    return rms * y / get_rms(y)\n",
    "\n",
    "\n",
    "regex_fn_wav = 'audio/ex*_unprocessed_input.wav'\n",
    "list_fn_wav = glob.glob(regex_fn_wav)\n",
    "\n",
    "example_waveforms = []\n",
    "for fn_wav in list_fn_wav:\n",
    "    sr, waveform = scipy.io.wavfile.read(fn_wav)\n",
    "    waveform = set_rms(waveform, 0.02)\n",
    "    assert sr == 20e3\n",
    "    example_waveforms.append(waveform)\n",
    "example_waveforms = np.array(example_waveforms)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for recognition_network_key in sorted(dict_recognition_network.keys()):\n",
    "        saver_0 = dict_recognition_network[recognition_network_key]['saver_0']\n",
    "        saver_0.restore(sess, dict_recognition_network[recognition_network_key]['fn_ckpt'])\n",
    "        saver_1 = dict_recognition_network[recognition_network_key]['saver_1']\n",
    "        saver_1.restore(sess, dict_recognition_network[recognition_network_key]['fn_ckpt'])\n",
    "\n",
    "    example_deep_feature_loss = sess.run(\n",
    "        tensor_deep_feature_loss,\n",
    "        feed_dict={\n",
    "            tensor_waveform_0: example_waveforms,\n",
    "            tensor_waveform_1: example_waveforms + np.random.randn(*example_waveforms.shape),\n",
    "        })\n",
    "\n",
    "example_deep_feature_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arch1_taskA': {'fn_arch': 'models/recognition_networks/arch1.json',\n",
       "  'fn_ckpt': 'models/recognition_networks/arch1_taskA.ckpt-550000',\n",
       "  'n_classes_dict': {'task_audioset': 517},\n",
       "  'saver_0': <tensorflow.python.training.saver.Saver at 0x2b7792ce2978>,\n",
       "  'saver_1': <tensorflow.python.training.saver.Saver at 0x2b723bea9a20>},\n",
       " 'arch2_taskA': {'fn_arch': 'models/recognition_networks/arch2.json',\n",
       "  'fn_ckpt': 'models/recognition_networks/arch2_taskA.ckpt-950000',\n",
       "  'n_classes_dict': {'task_audioset': 517},\n",
       "  'saver_0': <tensorflow.python.training.saver.Saver at 0x2b7793e8e898>,\n",
       "  'saver_1': <tensorflow.python.training.saver.Saver at 0x2b7752e6e6d8>},\n",
       " 'arch3_taskA': {'fn_arch': 'models/recognition_networks/arch3.json',\n",
       "  'fn_ckpt': 'models/recognition_networks/arch3_taskA.ckpt-980000',\n",
       "  'n_classes_dict': {'task_audioset': 517},\n",
       "  'saver_0': <tensorflow.python.training.saver.Saver at 0x2b77c4ec1710>,\n",
       "  'saver_1': <tensorflow.python.training.saver.Saver at 0x2b7793780be0>}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_recognition_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"arch1_taskA\": {\n",
      "        \"fn_arch\": \"models/recognition_networks/arch1.json\",\n",
      "        \"fn_ckpt\": \"models/recognition_networks/arch1_taskA.ckpt-550000\",\n",
      "        \"n_classes_dict\": {\n",
      "            \"task_audioset\": 517\n",
      "        },\n",
      "        \"weights\": {\n",
      "            \"batch_norm_0\": 7.721951988869115e-07,\n",
      "            \"batch_norm_1\": 2.180155442434395e-06,\n",
      "            \"batch_norm_2\": 4.35400902509891e-06,\n",
      "            \"batch_norm_3\": 9.663577649529144e-06,\n",
      "            \"batch_norm_4\": 7.037957926197431e-06,\n",
      "            \"batch_norm_5\": 8.04823966794683e-06,\n",
      "            \"batch_norm_6\": 9.601534802363368e-05\n",
      "        }\n",
      "    },\n",
      "    \"arch1_taskR\": {\n",
      "        \"fn_arch\": \"models/recognition_networks/arch1.json\",\n",
      "        \"fn_ckpt\": \"models/recognition_networks/arch1_taskR.ckpt-0\",\n",
      "        \"n_classes_dict\": {\n",
      "            \"task_word\": 794\n",
      "        },\n",
      "        \"weights\": {\n",
      "            \"batch_norm_0\": 4.5313309082274834e-06,\n",
      "            \"batch_norm_1\": 2.2921854969402775e-06,\n",
      "            \"batch_norm_2\": 9.444441099113525e-07,\n",
      "            \"batch_norm_3\": 4.3454074465209316e-07,\n",
      "            \"batch_norm_4\": 4.744116803227874e-07,\n",
      "            \"batch_norm_5\": 7.252991823836001e-07,\n",
      "            \"batch_norm_6\": 7.048356282228414e-07\n",
      "        }\n",
      "    },\n",
      "    \"arch1_taskW\": {\n",
      "        \"fn_arch\": \"models/recognition_networks/arch1.json\",\n",
      "        \"fn_ckpt\": \"models/recognition_networks/arch1_taskW.ckpt-360000\",\n",
      "        \"n_classes_dict\": {\n",
      "            \"task_word\": 794\n",
      "        },\n",
      "        \"weights\": {\n",
      "            \"batch_norm_0\": 8.148634005683363e-07,\n",
      "            \"batch_norm_1\": 2.655124304882748e-06,\n",
      "            \"batch_norm_2\": 5.720402614649923e-06,\n",
      "            \"batch_norm_3\": 1.7785463446355866e-05,\n",
      "            \"batch_norm_4\": 1.1564212308251202e-05,\n",
      "            \"batch_norm_5\": 1.3680587713195843e-05,\n",
      "            \"batch_norm_6\": 0.00012747735009257673\n",
      "        }\n",
      "    },\n",
      "    \"arch2_taskA\": {\n",
      "        \"fn_arch\": \"models/recognition_networks/arch2.json\",\n",
      "        \"fn_ckpt\": \"models/recognition_networks/arch2_taskA.ckpt-950000\",\n",
      "        \"n_classes_dict\": {\n",
      "            \"task_audioset\": 517\n",
      "        },\n",
      "        \"weights\": {\n",
      "            \"batch_norm_0\": 1.2787083010970974e-05,\n",
      "            \"batch_norm_1\": 3.872243390175745e-06,\n",
      "            \"batch_norm_2\": 4.0916914026823114e-05,\n",
      "            \"batch_norm_3\": 1.0983149329860448e-05,\n",
      "            \"batch_norm_4\": 1.6411619372836008e-05,\n",
      "            \"batch_norm_5\": 1.9605537700159232e-05,\n",
      "            \"batch_norm_6\": 8.89673192136142e-05,\n",
      "            \"batch_norm_7\": 0.00015303936142223106\n",
      "        }\n",
      "    },\n",
      "    \"arch2_taskR\": {\n",
      "        \"fn_arch\": \"models/recognition_networks/arch2.json\",\n",
      "        \"fn_ckpt\": \"models/recognition_networks/arch2_taskR.ckpt-0\",\n",
      "        \"n_classes_dict\": {\n",
      "            \"task_word\": 794\n",
      "        },\n",
      "        \"weights\": {\n",
      "            \"batch_norm_0\": 0.0009033044077854935,\n",
      "            \"batch_norm_1\": 0.001985916145766335,\n",
      "            \"batch_norm_2\": 0.04740769988747281,\n",
      "            \"batch_norm_3\": 0.049138172254876385,\n",
      "            \"batch_norm_4\": 0.09488066449971078,\n",
      "            \"batch_norm_5\": 0.14831164137971978,\n",
      "            \"batch_norm_6\": 0.8964458129175272,\n",
      "            \"batch_norm_7\": 2.0977237380635794\n",
      "        }\n",
      "    },\n",
      "    \"arch2_taskW\": {\n",
      "        \"fn_arch\": \"models/recognition_networks/arch2.json\",\n",
      "        \"fn_ckpt\": \"models/recognition_networks/arch2_taskW.ckpt-610000\",\n",
      "        \"n_classes_dict\": {\n",
      "            \"task_word\": 794\n",
      "        },\n",
      "        \"weights\": {\n",
      "            \"batch_norm_0\": 1.7039983238137272e-05,\n",
      "            \"batch_norm_1\": 4.049126793732267e-06,\n",
      "            \"batch_norm_2\": 5.5138894571540054e-05,\n",
      "            \"batch_norm_3\": 1.5994307965068624e-05,\n",
      "            \"batch_norm_4\": 2.3465571033206468e-05,\n",
      "            \"batch_norm_5\": 2.6932619594160208e-05,\n",
      "            \"batch_norm_6\": 9.091449999742952e-05,\n",
      "            \"batch_norm_7\": 0.0001247474154084437\n",
      "        }\n",
      "    },\n",
      "    \"arch3_taskA\": {\n",
      "        \"fn_arch\": \"models/recognition_networks/arch3.json\",\n",
      "        \"fn_ckpt\": \"models/recognition_networks/arch3_taskA.ckpt-980000\",\n",
      "        \"n_classes_dict\": {\n",
      "            \"task_audioset\": 517\n",
      "        },\n",
      "        \"weights\": {\n",
      "            \"batch_norm_0\": 9.367178190155468e-07,\n",
      "            \"batch_norm_1\": 4.047365064236076e-06,\n",
      "            \"batch_norm_2\": 3.221161721690842e-06,\n",
      "            \"batch_norm_3\": 1.6302867318651998e-06,\n",
      "            \"batch_norm_4\": 3.209753982180463e-06,\n",
      "            \"batch_norm_5\": 7.648978449406465e-06,\n",
      "            \"batch_norm_6\": 9.518763618955694e-06,\n",
      "            \"batch_norm_7\": 0.0001503918748524232\n",
      "        }\n",
      "    },\n",
      "    \"arch3_taskR\": {\n",
      "        \"fn_arch\": \"models/recognition_networks/arch3.json\",\n",
      "        \"fn_ckpt\": \"models/recognition_networks/arch3_taskR.ckpt-0\",\n",
      "        \"n_classes_dict\": {\n",
      "            \"task_word\": 794\n",
      "        },\n",
      "        \"weights\": {\n",
      "            \"batch_norm_0\": 8.944401019583153e-05,\n",
      "            \"batch_norm_1\": 0.0015361094702168968,\n",
      "            \"batch_norm_2\": 0.005594106590352144,\n",
      "            \"batch_norm_3\": 0.004969826705029876,\n",
      "            \"batch_norm_4\": 0.012617730321076999,\n",
      "            \"batch_norm_5\": 0.04534875913067663,\n",
      "            \"batch_norm_6\": 0.0649332731409872,\n",
      "            \"batch_norm_7\": 0.19110162724547713\n",
      "        }\n",
      "    },\n",
      "    \"arch3_taskW\": {\n",
      "        \"fn_arch\": \"models/recognition_networks/arch3.json\",\n",
      "        \"fn_ckpt\": \"models/recognition_networks/arch3_taskW.ckpt-580000\",\n",
      "        \"n_classes_dict\": {\n",
      "            \"task_word\": 794\n",
      "        },\n",
      "        \"weights\": {\n",
      "            \"batch_norm_0\": 8.376852943496161e-07,\n",
      "            \"batch_norm_1\": 4.130054862512673e-06,\n",
      "            \"batch_norm_2\": 4.30953902070214e-06,\n",
      "            \"batch_norm_3\": 2.510501905339445e-06,\n",
      "            \"batch_norm_4\": 5.482680435228746e-06,\n",
      "            \"batch_norm_5\": 1.2116750065352435e-05,\n",
      "            \"batch_norm_6\": 1.3421245766398092e-05,\n",
      "            \"batch_norm_7\": 0.0001430609287507201\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class AuditoryModelLoss():\n",
    "    def __init__(self,\n",
    "                 dir_recognition_network='models/recognition_networks'):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        fn_weights = os.path.join(dir_recognition_network, 'deep_feature_loss_weights.json')\n",
    "        with open(fn_weights, 'r') as f_weights:\n",
    "            deep_feature_loss_weights = json.load(f_weights)\n",
    "        list_fn_ckpt = glob.glob(os.path.join(dir_recognition_network, '*ckpt*index'))\n",
    "        list_fn_ckpt = [fn_ckpt.replace('.index', '') for fn_ckpt in list_fn_ckpt]\n",
    "        dict_recognition_network = {}\n",
    "        for fn_ckpt in list_fn_ckpt:\n",
    "            recognition_network_key = os.path.basename(fn_ckpt).split('.')[0]\n",
    "            if 'taskA' in recognition_network_key:\n",
    "                n_classes_dict = {\"task_audioset\": 517}\n",
    "            else:\n",
    "                n_classes_dict = {\"task_word\": 794}\n",
    "            dict_recognition_network[recognition_network_key] = {\n",
    "                'fn_ckpt': fn_ckpt,\n",
    "                'fn_arch': fn_ckpt[:fn_ckpt.rfind('_task')] + '.json',\n",
    "                'n_classes_dict': n_classes_dict,\n",
    "                'weights': deep_feature_loss_weights[recognition_network_key],\n",
    "            }\n",
    "        print(json.dumps(dict_recognition_network, indent=4, sort_keys=True))\n",
    "    \n",
    "    \n",
    "    def build_auditory_model():\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def load_auditory_model():\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def waveform_loss(y0, y1):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def cochlear_model_loss(y0, y1):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def deep_feature_loss(y0, y1):\n",
    "        pass\n",
    "\n",
    "\n",
    "auditory_model_loss = AuditoryModelLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
