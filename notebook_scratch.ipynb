{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io.wavfile\n",
    "import IPython.display as ipd\n",
    "\n",
    "import util_recognition_network\n",
    "import util_cochlear_model\n",
    "import util_auditory_model_loss\n",
    "import util_audio_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCH = 1\n",
    "TASK = 'A'\n",
    "\n",
    "fn_ckpt = 'models/recognition_networks/arch{}_task{}.ckpt*'.format(ARCH, TASK)\n",
    "fn_ckpt = glob.glob(fn_ckpt)[-1].replace('.index', '')\n",
    "fn_arch = 'models/recognition_networks/arch{}.json'.format(ARCH)\n",
    "with open(fn_arch, 'r') as f_arch:\n",
    "    list_layer_dict = json.load(f_arch)\n",
    "\n",
    "fn_ckpt, fn_arch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "importlib.reload(util_recognition_network)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "if 'taskA' in fn_ckpt:\n",
    "    n_classes_dict = {\"/stimuli/labels_binary_via_int\": 517}\n",
    "else:\n",
    "    n_classes_dict = {\"/stimuli/word_int\": 794}\n",
    "\n",
    "\n",
    "tensor_input = tf.placeholder(tf.float32, [None, 40, 20000, 1])\n",
    "tensor_output, tensors = util_recognition_network.build_network(\n",
    "    tensor_input,\n",
    "    list_layer_dict,\n",
    "    n_classes_dict=n_classes_dict)\n",
    "\n",
    "var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=None)\n",
    "var_dict = {v.name: v for v in var_list}\n",
    "\n",
    "saver = tf.train.Saver(var_list=var_list, max_to_keep=0)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    var_dict_init = sess.run(var_dict)\n",
    "    saver.restore(sess, fn_ckpt)\n",
    "    var_dict_load = sess.run(var_dict)\n",
    "\n",
    "for v in var_list:\n",
    "    print(v.name, v.shape, np.mean(np.abs(var_dict_init[v.name] - var_dict_load[v.name])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rms(y):\n",
    "    return np.sqrt(np.mean(np.square(y - np.mean(y))))\n",
    "\n",
    "def set_rms(y, rms):\n",
    "    y = y - np.mean(y)\n",
    "    return rms * y / get_rms(y)\n",
    "\n",
    "\n",
    "regex_fn_wav = 'audio/ex*_unprocessed_input.wav'\n",
    "list_fn_wav = glob.glob(regex_fn_wav)\n",
    "\n",
    "list_y = []\n",
    "for fn_wav in list_fn_wav:\n",
    "    sr, y = scipy.io.wavfile.read(fn_wav)\n",
    "    y = set_rms(y, 0.02)\n",
    "    assert sr == 20e3\n",
    "    list_y.append(y)\n",
    "list_y = np.array(list_y)\n",
    "list_y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(util_cochlear_model)\n",
    "importlib.reload(util_audio_transform)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "fn_ckpt = 'models/audio_transforms/unet_cochlear_reverse/model.ckpt-600000'\n",
    "\n",
    "tensor_waveform = tf.placeholder(tf.float32, [None, 40000])\n",
    "\n",
    "tensor_cochlear_representation, coch_container = util_cochlear_model.build_cochlear_model(\n",
    "    tensor_waveform,\n",
    "    signal_rate=20000,\n",
    "    filter_type='half-cosine',\n",
    "    filter_spacing='erb',\n",
    "    HIGH_LIM=8000,\n",
    "    LOW_LIM=20,\n",
    "    N=40,\n",
    "    SAMPLE_FACTOR=1,\n",
    "    bandwidth_scale_factor=1.0,\n",
    "    compression='stable_point3',\n",
    "    include_highpass=False,\n",
    "    include_lowpass=False,\n",
    "    linear_max=1.0,\n",
    "    rFFT=True,\n",
    "    rectify_and_lowpass_subbands=True,\n",
    "    return_subbands_only=True)\n",
    "\n",
    "tensor_waveform_unet = util_audio_transform.build_unet(\n",
    "    tensor_waveform,\n",
    "    signal_rate=20000,\n",
    "    UNET_PARAMS={})\n",
    "\n",
    "var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=None)\n",
    "saver = tf.train.Saver(var_list=var_list, max_to_keep=0)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, fn_ckpt)\n",
    "    list_y_unet = sess.run(tensor_waveform_unet, feed_dict={tensor_waveform: list_y})\n",
    "    list_coch_y = sess.run(tensor_cochlear_representation, feed_dict={tensor_waveform: list_y})\n",
    "    list_coch_y_unet = sess.run(tensor_cochlear_representation, feed_dict={tensor_waveform: list_y_unet})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for itr0 in range(list_y_unet.shape[0]):\n",
    "    y = list_y[itr0]\n",
    "    y_unet = list_y_unet[itr0]\n",
    "    print(list_fn_wav[itr0], np.mean(list_coch_y[itr0]), np.mean(list_coch_y_unet[itr0]))\n",
    "    ipd.display(ipd.Audio(y, rate=sr))\n",
    "    ipd.display(ipd.Audio(y_unet, rate=sr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_recognition_network_ckpt = 'models/recognition_networks/arch*taskA*ckpt*index'\n",
    "fn_deep_feature_loss_weights= 'models/recognition_networks/deep_feature_loss_weights.json'\n",
    "\n",
    "list_recognition_network_ckpt = [\n",
    "    fn_ckpt.replace('.index', '') for fn_ckpt in glob.glob(regex_recognition_network_ckpt)\n",
    "]\n",
    "\n",
    "with open(fn_deep_feature_loss_weights, 'r') as f:\n",
    "    deep_feature_loss_weights = json.load(f)\n",
    "\n",
    "dict_recognition_network = {}\n",
    "for fn_ckpt in list_recognition_network_ckpt:\n",
    "    key = os.path.basename(fn_ckpt).split('.')[0]\n",
    "    if 'taskA' in key:\n",
    "        n_classes_dict = {\"task_audioset\": 517}\n",
    "    else:\n",
    "        n_classes_dict = {\"task_word\": 794}\n",
    "    dict_recognition_network[key] = {\n",
    "        'fn_ckpt': fn_ckpt,\n",
    "        'fn_arch': fn_ckpt[:fn_ckpt.rfind('_task')] + '.json',\n",
    "        'n_classes_dict': n_classes_dict,\n",
    "    }\n",
    "\n",
    "dict_recognition_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(util_audio_transform)\n",
    "importlib.reload(util_cochlear_model)\n",
    "importlib.reload(util_recognition_network)\n",
    "\n",
    "\n",
    "kwargs_build_cochlear_model = {}\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tensor_waveform_0 = tf.placeholder(tf.float32, [None, 40000])\n",
    "tensor_waveform_1 = tf.placeholder(tf.float32, [None, 40000])\n",
    "tensor_deep_feature_loss = tf.zeros([], dtype=tf.float32)\n",
    "\n",
    "tensor_coch_0, _ = util_cochlear_model.build_cochlear_model(\n",
    "    tensor_waveform_0,\n",
    "    **kwargs_build_cochlear_model)\n",
    "tensor_coch_1, _ = util_cochlear_model.build_cochlear_model(\n",
    "    tensor_waveform_1,\n",
    "    **kwargs_build_cochlear_model)\n",
    "\n",
    "\n",
    "for recognition_network_key in sorted(dict_recognition_network.keys()):\n",
    "    with open(dict_recognition_network[recognition_network_key]['fn_arch'], 'r') as f:\n",
    "        list_layer_dict = json.load(f)\n",
    "    with tf.variable_scope(recognition_network_key + '_0') as scope:\n",
    "        _, recognition_network_tensors_0 = util_recognition_network.build_network(\n",
    "            tensor_coch_0,\n",
    "            list_layer_dict,\n",
    "            n_classes_dict=dict_recognition_network[key]['n_classes_dict'])\n",
    "        var_list = {\n",
    "            v.name.replace(scope.name + '/', '').replace(':0', ''): v\n",
    "            for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope.name)\n",
    "        }\n",
    "        dict_recognition_network[recognition_network_key]['saver_0'] = tf.train.Saver(\n",
    "            var_list=var_list,\n",
    "            max_to_keep=0)\n",
    "    with tf.variable_scope(recognition_network_key + '_1') as scope:\n",
    "        _, recognition_network_tensors_1 = util_recognition_network.build_network(\n",
    "            tensor_coch_1,\n",
    "            list_layer_dict,\n",
    "            n_classes_dict=dict_recognition_network[key]['n_classes_dict'])\n",
    "        var_list = {\n",
    "            v.name.replace(scope.name + '/', '').replace(':0', ''): v\n",
    "            for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope.name)\n",
    "        }\n",
    "        dict_recognition_network[recognition_network_key]['saver_1'] = tf.train.Saver(\n",
    "            var_list=var_list,\n",
    "            max_to_keep=0)\n",
    "    \n",
    "    for feature_key in sorted(deep_feature_loss_weights[recognition_network_key].keys()):\n",
    "        feature_weight = deep_feature_loss_weights[recognition_network_key][feature_key]\n",
    "        feature_0 = recognition_network_tensors_0[feature_key]\n",
    "        feature_1 = recognition_network_tensors_1[feature_key]\n",
    "\n",
    "        feature_l1_distance = tf.reduce_sum(\n",
    "            tf.math.abs(feature_0 - feature_1),\n",
    "            axis=np.arange(1, len(feature_0.get_shape().as_list())))\n",
    "\n",
    "        tensor_deep_feature_loss += feature_weight * feature_l1_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rms(y):\n",
    "    return np.sqrt(np.mean(np.square(y - np.mean(y))))\n",
    "\n",
    "def set_rms(y, rms):\n",
    "    y = y - np.mean(y)\n",
    "    return rms * y / get_rms(y)\n",
    "\n",
    "\n",
    "regex_fn_wav = 'audio/ex*_unprocessed_input.wav'\n",
    "list_fn_wav = glob.glob(regex_fn_wav)\n",
    "\n",
    "example_waveforms = []\n",
    "for fn_wav in list_fn_wav:\n",
    "    sr, waveform = scipy.io.wavfile.read(fn_wav)\n",
    "    waveform = set_rms(waveform, 0.02)\n",
    "    assert sr == 20e3\n",
    "    example_waveforms.append(waveform)\n",
    "example_waveforms = np.array(example_waveforms)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for recognition_network_key in sorted(dict_recognition_network.keys()):\n",
    "        saver_0 = dict_recognition_network[recognition_network_key]['saver_0']\n",
    "        saver_0.restore(sess, dict_recognition_network[recognition_network_key]['fn_ckpt'])\n",
    "        saver_1 = dict_recognition_network[recognition_network_key]['saver_1']\n",
    "        saver_1.restore(sess, dict_recognition_network[recognition_network_key]['fn_ckpt'])\n",
    "\n",
    "    example_deep_feature_loss = sess.run(\n",
    "        tensor_deep_feature_loss,\n",
    "        feed_dict={\n",
    "            tensor_waveform_0: example_waveforms,\n",
    "            tensor_waveform_1: example_waveforms + np.random.randn(*example_waveforms.shape),\n",
    "        })\n",
    "\n",
    "example_deep_feature_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_recognition_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuditoryModelLoss():\n",
    "    def __init__(self,\n",
    "                 dir_recognition_networks='models/recognition_networks',\n",
    "                 config_cochlear_model={}):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        fn_weights = os.path.join(dir_recognition_networks, 'deep_feature_loss_weights.json')\n",
    "        with open(fn_weights, 'r') as f_weights:\n",
    "            deep_feature_loss_weights = json.load(f_weights)\n",
    "        list_fn_ckpt = glob.glob(os.path.join(dir_recognition_networks, 'arch1*ckpt*index'))\n",
    "        list_fn_ckpt = [fn_ckpt.replace('.index', '') for fn_ckpt in list_fn_ckpt]\n",
    "        config_recognition_networks = {}\n",
    "        for fn_ckpt in list_fn_ckpt:\n",
    "            network_key = os.path.basename(fn_ckpt).split('.')[0]\n",
    "            if 'taskA' in network_key:\n",
    "                n_classes_dict = {\"task_audioset\": 517}\n",
    "            else:\n",
    "                n_classes_dict = {\"task_word\": 794}\n",
    "            config_recognition_networks[network_key] = {\n",
    "                'fn_ckpt': fn_ckpt,\n",
    "                'fn_arch': fn_ckpt[:fn_ckpt.rfind('_task')] + '.json',\n",
    "                'n_classes_dict': n_classes_dict,\n",
    "                'weights': deep_feature_loss_weights[network_key],\n",
    "            }\n",
    "        self.config_recognition_networks = config_recognition_networks\n",
    "        self.config_cochlear_model = config_cochlear_model\n",
    "        self.build_auditory_model()\n",
    "        self.sess = None\n",
    "        self.vars_loaded = False\n",
    "\n",
    "\n",
    "    def l1_distance(self, feature0, feature1):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        axis = np.arange(1, len(feature0.get_shape().as_list()))\n",
    "        return tf.reduce_sum(tf.math.abs(feature0 - feature1), axis=axis)\n",
    "\n",
    "\n",
    "    def build_auditory_model(self, dtype=tf.float32):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Build placeholders for two waveforms and compute waveform loss\n",
    "        self.tensor_wave0 = tf.placeholder(dtype, [None, 40000])\n",
    "        self.tensor_wave1 = tf.placeholder(dtype, [None, 40000])\n",
    "        print('Building waveform loss')\n",
    "        self.loss_waveform = self.l1_distance(self.tensor_wave0, self.tensor_wave1)\n",
    "        # Build cochlear model for each waveform and compute cochlear model loss\n",
    "        tensor_coch0, _ = util_cochlear_model.build_cochlear_model(\n",
    "            self.tensor_wave0,\n",
    "            **self.config_cochlear_model)\n",
    "        tensor_coch1, _ = util_cochlear_model.build_cochlear_model(\n",
    "            self.tensor_wave1,\n",
    "            **self.config_cochlear_model)\n",
    "        print('Building cochlear model loss')\n",
    "        self.loss_cochlear_model = self.l1_distance(tensor_coch0, tensor_coch1)\n",
    "        # Build network(s) for each waveform and compute deep feature losses\n",
    "        self.loss_deep_features_dict = {}\n",
    "        self.loss_deep_features = tf.zeros([], dtype=dtype)\n",
    "        for network_key in sorted(self.config_recognition_networks.keys()):\n",
    "            with open(self.config_recognition_networks[network_key]['fn_arch'], 'r') as f:\n",
    "                list_layer_dict = json.load(f)\n",
    "            # Build network for stimulus 0\n",
    "            with tf.variable_scope(network_key + '0') as scope:\n",
    "                _, tensors_network0 = util_recognition_network.build_network(\n",
    "                    tensor_coch0,\n",
    "                    list_layer_dict,\n",
    "                    n_classes_dict=self.config_recognition_networks[network_key]['n_classes_dict'])\n",
    "                var_list = {\n",
    "                    v.name.replace(scope.name + '/', '').replace(':0', ''): v\n",
    "                    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope.name)\n",
    "                }\n",
    "                self.config_recognition_networks[network_key]['saver0'] = tf.train.Saver(\n",
    "                    var_list=var_list,\n",
    "                    max_to_keep=0)\n",
    "            # Build network for stimulus 1\n",
    "            with tf.variable_scope(network_key + '1') as scope:\n",
    "                _, tensors_network1 = util_recognition_network.build_network(\n",
    "                    tensor_coch1,\n",
    "                    list_layer_dict,\n",
    "                    n_classes_dict=self.config_recognition_networks[network_key]['n_classes_dict'])\n",
    "                var_list = {\n",
    "                    v.name.replace(scope.name + '/', '').replace(':0', ''): v\n",
    "                    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope.name)\n",
    "                }\n",
    "                self.config_recognition_networks[network_key]['saver1'] = tf.train.Saver(\n",
    "                    var_list=var_list,\n",
    "                    max_to_keep=0)\n",
    "            # Compute deep feature losses (weighted sum across layers)\n",
    "            print('Building deep feature loss (recognition network: {})'.format(network_key))\n",
    "            self.loss_deep_features_dict[network_key] = tf.zeros([], dtype=dtype)\n",
    "            layer_weights = self.config_recognition_networks[network_key]['weights']\n",
    "            for layer_key in sorted(layer_weights.keys()):\n",
    "                tmp = self.l1_distance(tensors_network0[layer_key], tensors_network1[layer_key])\n",
    "                self.loss_deep_features_dict[network_key] += layer_weights[layer_key] * tmp\n",
    "            self.loss_deep_features += self.loss_deep_features_dict[network_key]\n",
    "\n",
    "\n",
    "    def load_auditory_model_vars(self, sess):\n",
    "        self.sess = sess\n",
    "        for network_key in sorted(self.config_recognition_networks.keys()):\n",
    "            fn_ckpt = self.config_recognition_networks[network_key]['fn_ckpt']\n",
    "            saver0 = self.config_recognition_networks[network_key]['saver0']\n",
    "            saver1 = self.config_recognition_networks[network_key]['saver1']\n",
    "            print('Loading `{}` variables from {}'.format(network_key, fn_ckpt))\n",
    "            saver0.restore(self.sess, fn_ckpt)\n",
    "            saver1.restore(self.sess, fn_ckpt)\n",
    "        self.vars_loaded = True\n",
    "\n",
    "\n",
    "    def waveform_loss(self, y0, y1):\n",
    "        assert (self.sess is not None) and (not self.sess._closed)\n",
    "        feed_dict={self.tensor_wave0: y0, self.tensor_wave1: y1}\n",
    "        return self.sess.run(self.loss_waveform, feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "    def cochlear_model_loss(self, y0, y1):\n",
    "        assert (self.sess is not None) and (not self.sess._closed)\n",
    "        feed_dict={self.tensor_wave0: y0, self.tensor_wave1: y1}\n",
    "        return self.sess.run(self.loss_cochlear_model, feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "    def deep_feature_loss(self, y0, y1):\n",
    "        assert (self.sess is not None) and (not self.sess._closed)\n",
    "        if not self.vars_loaded:\n",
    "            print((\"WARNING: `deep_feature_loss` called before loading vars\"))\n",
    "        feed_dict={self.tensor_wave0: y0, self.tensor_wave1: y1}\n",
    "        return self.sess.run(self.loss_deep_features, feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rms(y):\n",
    "    return np.sqrt(np.mean(np.square(y - np.mean(y))))\n",
    "\n",
    "def set_rms(y, rms):\n",
    "    y = y - np.mean(y)\n",
    "    return rms * y / get_rms(y)\n",
    "\n",
    "\n",
    "regex_fn_wav = 'audio/ex*_unprocessed_input.wav'\n",
    "list_fn_wav = glob.glob(regex_fn_wav)\n",
    "\n",
    "example_waveforms = []\n",
    "for fn_wav in list_fn_wav:\n",
    "    sr, waveform = scipy.io.wavfile.read(fn_wav)\n",
    "    waveform = set_rms(waveform, 0.02)\n",
    "    assert sr == 20e3\n",
    "    example_waveforms.append(waveform)\n",
    "example_waveforms = np.array(example_waveforms)\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "auditory_model_loss = AuditoryModelLoss()\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "with tf.Session() as sess:\n",
    "    auditory_model_loss.load_auditory_model_vars(sess)\n",
    "    \n",
    "    print(auditory_model_loss.waveform_loss(example_waveforms[0:1], example_waveforms[1:2]))\n",
    "    print(auditory_model_loss.waveform_loss(example_waveforms, example_waveforms))\n",
    "    print(auditory_model_loss.cochlear_model_loss(example_waveforms[0:1], example_waveforms[1:2]))\n",
    "    print(auditory_model_loss.cochlear_model_loss(example_waveforms, example_waveforms))\n",
    "    print(auditory_model_loss.deep_feature_loss(example_waveforms[0:1], example_waveforms[1:2]))\n",
    "    print(auditory_model_loss.deep_feature_loss(example_waveforms[0:1], example_waveforms[0:1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auditory_model_loss.loss_deep_features, auditory_model_loss.loss_deep_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auditory_model_loss.vars_loaded, auditory_model_loss.sess._closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util_auditory_model_loss\n",
    "auditory_model_loss = util_auditory_model_loss.AuditoryModelLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
